import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
# Ensure anova_glm or manual LRT calculation code has been run to identify interactions
# We will use the list 'potential_interactions_to_retain' generated by the previous code.
# We also need X_df_aligned and y_binomial from previous steps.

# IMPORTANT: Ensure 'potential_interactions_to_retain', 'X_df_aligned', and 'y_binomial'
# are available from prior execution of the manual LRT code.
required_vars = ['potential_interactions_to_retain', 'X_df_aligned', 'y_binomial']
if not all(v in locals() or v in globals() for v in required_vars):
    raise NameError(f"Missing required variables: {', '.join([v for v in required_vars if v not in locals() and v not in globals()])}. Please run the interaction testing code first.")

print("\n--- Building Final Primary Model with Interactions ---")

# Start with the main effects predictor DataFrame
X_final_primary = X_df_aligned.copy()

# Define a helper function to create the unique interaction column name
def get_interaction_col_name(eth_col, interact_col):
    return f"{eth_col}_x_{interact_col}".replace('race_', '').replace('time_period_', '')


# Add each retained interaction term to the DataFrame
print(f"Adding {len(potential_interactions_to_retain)} interaction terms to the final model:")
for interaction_name_print in potential_interactions_to_retain:
    # Parse the interaction name to get the two original column names
    # Assuming the format is 'col1 * col2'
    parts = interaction_name_print.split(' * ')
    if len(parts) == 2:
        eth_col = parts[0]
        interact_col = parts[1]
        interaction_col_name_unique = get_interaction_col_name(eth_col, interact_col)

        # Ensure the base columns exist before creating the interaction
        if eth_col in X_df_aligned.columns and interact_col in X_df_aligned.columns:
             X_final_primary[interaction_col_name_unique] = X_df_aligned[eth_col] * X_df_aligned[interact_col]
             print(f"- Added interaction: {interaction_name_print} (Column: {interaction_col_name_unique})")
        else:
             print(f"Warning: Base columns '{eth_col}' or '{interact_col}' not found for interaction '{interaction_name_print}'. Skipping addition.")
    else:
        print(f"Warning: Could not parse interaction name '{interaction_name_print}'. Skipping addition.")


# --- Fit the Final Primary Binomial GLM ---
print("\nFitting the Final Primary Binomial GLM...")
# Ensure column names are strings before fitting
X_final_primary.columns = X_final_primary.columns.astype(str)
final_primary_model_cols = X_final_primary.columns.tolist()


try:
    final_primary_model = sm.GLM(y_binomial, X_final_primary, family=sm.families.Binomial())
    # You might consider adding disp=0 if convergence issues arise in the final model,
    # but it's often best to try without first.
    final_primary_result = final_primary_model.fit()

    print("\n--- Final Primary Model Summary ---")
    print(final_primary_result.summary(xname=final_primary_model_cols))

    print("\n--- Final Primary Model Odds Ratios (with Interactions) ---")
    try:
        odds_ratios=np.exp(final_primary_result.params); conf_int=np.exp(final_primary_result.conf_int()); p_values=final_primary_result.pvalues
        or_df=pd.DataFrame({'Var': odds_ratios.index, 'OR': odds_ratios.values, 'CIL': conf_int.iloc[:,0].values, 'CIU': conf_int.iloc[:,1].values, 'p': p_values.values})
        or_df['OR (95% CI)'] = or_df.apply(lambda r: f"{r['OR']:.2f} ({r['CIL']:.2f}-{r['CIU']:.2f})" if pd.notna(r['OR']) else "N/A", axis=1)
        # Print only columns of interest
        print(or_df[['Var', 'OR (95% CI)', 'p']])
    except Exception as e: print(f"Error generating ORs for final model: {e}")


    # You should re-run model diagnostics on this final model:
    # - Hosmer-Lemeshow test (likely still poor p-value due to sample size, but check)
    # - Binned calibration plot (visual check)
    # - VIFs (check for multicollinearity introduced by interactions)
    # - Residuals/Cook's Distance (if not done previously)

    # Note: Re-running the calibration plot code with final_primary_result and X_final_primary
    # would be a good idea, though the plot structure will be the same.

except Exception as e:
    print(f"\nError fitting the final primary model: {e}")


print("\n--- Final Primary Model Building Complete ---")

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import scipy.stats as stats # For chi-squared p-value in HL test
from statsmodels.stats.outliers_influence import variance_inflation_factor # For VIFs

# --- Assume final_primary_result and X_final_primary exist from the previous step ---
# final_primary_result: The fitted GLM result object for the final primary model
# X_final_primary: DataFrame with predictors for the final primary model (includes 'const')
# model_df_aligned: DataFrame with aligned patient data (needed for observed outcome)
# result: The main effects model result (needed if comparing AICs, but not for diagnostics on final model itself)

# IMPORTANT: Ensure 'final_primary_result', 'X_final_primary', and 'model_df_aligned'
# are available from prior execution.
required_vars = ['final_primary_result', 'X_final_primary', 'model_df_aligned']
if not all(v in locals() or v in globals() for v in required_vars):
    raise NameError(f"Missing required variables: {', '.join([v for v in required_vars if v not in locals() and v not in globals()])}. Please run the previous code blocks first.")

print("\n--- Performing Diagnostics on Final Primary Model ---")

# Get predictions from the final model
predicted_proportions_final = final_primary_result.predict(X_final_primary)

# Align predictions with observed data (restraint days and length of stay)
common_index = model_df_aligned.index.intersection(X_final_primary.index)
model_df_aligned_for_diagnostics = model_df_aligned.loc[common_index].copy() # Ensure alignment
predicted_proportions_aligned = predicted_proportions_final.loc[common_index]

# Add predictions to the aligned DataFrame for convenience
model_df_aligned_for_diagnostics['predicted_proportion_final'] = predicted_proportions_aligned

# --- Add the calculation for observed proportion back ---
model_df_aligned_for_diagnostics['observed_proportion'] = model_df_aligned_for_diagnostics['restraint_calendar_days'] / model_df_aligned_for_diagnostics['length_of_stay'].replace(0, np.nan)


# --- 1. Hosmer-Lemeshow Test ---
print("\n--- Hosmer-Lemeshow Test (Final Model) ---")
hl_test_possible = True
# Check for required columns and NaNs in the prepared diagnostics df
required_hl_cols = ['restraint_calendar_days', 'length_of_stay', 'predicted_proportion_final', 'observed_proportion']
if not all(col in model_df_aligned_for_diagnostics.columns for col in required_hl_cols) or model_df_aligned_for_diagnostics[required_hl_cols].isnull().any().any():
     print("HL Test skipped: Dataframe for diagnostics missing columns or contains NaNs where not expected.")
     hl_test_possible = False
else:
    # Use the prepared data, ensuring no NaNs in crucial columns for HL/Plot
    hl_data = model_df_aligned_for_diagnostics[required_hl_cols].dropna().copy()


    if hl_data.empty or hl_data['length_of_stay'].sum() == 0:
        print("HL Test skipped: No valid data points with non-zero length of stay after filtering.")
        hl_test_possible = False
    else:
        try:
            # Ensure predicted proportion is within [0, 1] for qcut
            hl_data['predicted_proportion_final'] = hl_data['predicted_proportion_final'].clip(0, 1)

            # Use qcut to create bins based on predicted probabilities
            n_bins_hl = 10
            # Use labels=False to get integer bin indices, duplicates='drop' handles identical predictions
            # Ensure enough data points relative to n_bins_hl
            if len(hl_data) < n_bins_hl * 2: # Simple check: need at least 2x bins data points
                 print(f"HL Test Warning: Only {len(hl_data)} data points for binning, which is less than {n_bins_hl*2} (2x num_bins). May lead to unstable bins.")


            hl_data['hl_group'] = pd.qcut(hl_data['predicted_proportion_final'], q=n_bins_hl, labels=False, duplicates='drop')

            G = hl_data['hl_group'].nunique()
            print(f"HL using {G} groups.")

            if G < 2:
                print("HL Test skipped: Fewer than 2 unique groups created.")
                hl_test_possible = False
            elif G <= 2:
                 print("HL Test Warning: Only 2 groups created. HL test may be less reliable.")
            else:
                # Group data by the created bins
                # observed=True for newer pandas versions
                hl_grouped = hl_data.groupby('hl_group', observed=True).agg(
                    O_g=('restraint_calendar_days','sum'), # Observed successes (restraint days)
                    N_g=('length_of_stay','sum'),        # Number of trials (total days in bin)
                    mean_predicted=('predicted_proportion_final', 'mean') # Mean predicted probability in bin
                )

                # Calculate expected successes and failures in each bin
                hl_grouped['E_g'] = hl_grouped['mean_predicted'] * hl_grouped['N_g'] # Expected successes
                hl_grouped['O_0g'] = hl_grouped['N_g'] - hl_grouped['O_g'] # Observed failures
                hl_grouped['E_0g'] = hl_grouped['N_g'] - hl_grouped['E_g'] # Expected failures

                # Calculate the HL statistic
                # Add a small epsilon to denominators to avoid division by zero if E_g or E_0g are exactly 0
                epsilon = 1e-9
                term1 = ((hl_grouped['O_g'] - hl_grouped['E_g'])**2 / (hl_grouped['E_g'] + epsilon)).fillna(0)
                term2 = ((hl_grouped['O_0g'] - hl_grouped['E_0g'])**2 / (hl_grouped['E_0g'] + epsilon)).fillna(0)

                hl_stat = (term1 + term2).sum()

                # Degrees of freedom is number of groups minus 2 (for intercept and slope)
                # Only calculate if G > 2, otherwise DF <= 0
                hl_df = G - 2


                if hl_df <= 0:
                    print("HL Test skipped: Degrees of freedom <= 0.") # Should already be caught by G < 2, but safety check
                    hl_test_possible = False # Ensure plot also skipped if HL not possible
                else:
                    # Calculate p-value using chi-squared survival function
                    hl_p = stats.chi2.sf(hl_stat, hl_df)

                    print(f"\nHosmer-Lemeshow Results:");
                    print(f"  HL Stat: {hl_stat:.4f}, DF: {hl_df}, P-val: {hl_p:.4f}");
                    print(f"  Interpretation: {'Good fit (p>0.05)' if hl_p > 0.05 else 'POOR fit (p<=0.05)'}")
                    # Print group details for inspection if needed
                    # print("\nHL Group Summary:"); print(hl_grouped)

        except Exception as e:
            print(f"HL Test Error: {e}")
            hl_test_possible = False


# --- 2. Binned Calibration Plot ---
print("\n--- Binned Calibration Plot (Final Model) ---")
# Proceed with plot if HL data was successfully prepared and grouped
if hl_test_possible and not hl_data.empty and 'hl_group' in hl_data.columns:
    try:
        # Recalculate mean observed and predicted per bin using the HL groups
        # Need to calculate observed proportion within the aggregated data if not done earlier
        # Use the hl_data DataFrame that already has 'observed_proportion'
        calibration_plot_data = hl_data.groupby('hl_group', observed=True).agg(
            mean_predicted=('predicted_proportion_final', 'mean'),
            mean_observed=('observed_proportion', 'mean'), # Use the observed_proportion column
            n_patients=('predicted_proportion_final', 'count') # Count patients in each bin
        ).reset_index()


        plt.figure(figsize=(8, 8))

        scatter = plt.scatter(
            calibration_plot_data['mean_predicted'],
            calibration_plot_data['mean_observed'],
            s=calibration_plot_data['n_patients'] / calibration_plot_data['n_patients'].max() * 400 + 30,
            alpha=0.7,
            label=f'{G} Bins (Size proportional to patient count)'
        )

        plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')

        plt.xlabel("Mean Predicted Proportion of Restraint Days (Final Model)")
        plt.ylabel("Mean Observed Proportion of Restraint Days")
        plt.title("Binned Calibration Plot (Final Model)")
        plt.legend()
        plt.grid(True, alpha=0.6)
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.gca().set_aspect('equal', adjustable='box')

        plt.show()

    except Exception as e:
        print(f"Binned Calibration Plot Error: {e}")
else:
    print("Binned Calibration Plot skipped due to issues with data preparation or grouping.")


# --- 3. Variance Inflation Factors (VIFs) ---
print("\n--- Variance Inflation Factors (VIFs) for Final Model Predictors ---")
# Drop the constant term for VIF calculation
X_vif = X_final_primary.drop(columns='const', errors='ignore').copy()

# Ensure all columns are numeric and handle potential inf/NaN
# Use the common_index to ensure alignment if needed, though X_final_primary should be clean
X_vif = X_vif.loc[common_index].apply(pd.to_numeric, errors='coerce').fillna(0) # Fillna(0) for safety, though dropna(subset) earlier should minimize issues

# Remove columns with zero or near-zero variance, as VIF is undefined
variances = X_vif.var()
low_var_cols = variances[variances <= 1e-10].index.tolist()
if low_var_cols:
    print(f"Warning: Dropping low variance columns for VIF calculation: {low_var_cols}")
    X_vif = X_vif.drop(columns=low_var_cols)

if not X_vif.empty and X_vif.shape[1] > 1:
    vif_data = []
    # Ensure input to variance_inflation_factor is a numpy array and finite
    numpy_X_vif = X_vif.to_numpy()
    if np.isfinite(numpy_X_vif).all():
        try:
            # Use this loop for VIF calculation
            for i in range(numpy_X_vif.shape[1]):
                vif_val = variance_inflation_factor(numpy_X_vif, i)
                vif_data.append({"Var": X_vif.columns[i], "VIF": vif_val})

            vif_df = pd.DataFrame(vif_data).round(2)
            print(vif_df)

            # Optional: Highlight high VIFs
            high_vif_threshold = 5 # Common threshold
            high_vif_vars = vif_df[vif_df['VIF'] > high_vif_threshold]
            if not high_vif_vars.empty:
                print(f"\nWarning: Variables with VIF > {high_vif_threshold}:")
                print(high_vif_vars)

        except Exception as e:
            print(f"Error calculating VIFs: {e}")
            print("This can happen with perfectly collinear columns even after removing zero variance columns.")

    else:
         print("VIF calculation skipped: Non-finite values detected in predictor matrix.")
else:
    print("Not enough data or variables (>1) for VIF calculation after removing constant and low-variance columns.")


# --- 4. Residuals and Cook's Distance (Basic Check) ---
print("\n--- Residuals and Cook's Distance (Final Model) ---")
try:
    # Get Pearson residuals - These should already be aligned NumPy arrays
    pearson_residuals = final_primary_result.resid_pearson # Removed .loc[common_index]

    # Get Cook's distance - This should already be an aligned NumPy array
    influence = final_primary_result.get_influence()
    cooks_d = influence.cooks_distance[0] # Removed .loc[common_index]

    print(f"Summary of Pearson Residuals:\n{pd.Series(pearson_residuals).describe()}") # Convert to Series for describe
    print(f"\nSummary of Cook's Distance:\n{pd.Series(cooks_d).describe()}") # Convert to Series for describe

    # Optional: Identify potentially influential points
    # Common threshold is 4/N
    N = len(cooks_d)
    cooks_threshold = 4 / N
    # Use the boolean mask on the aligned DataFrame's index to get the original indices
    influential_points_indices = model_df_aligned_for_diagnostics.index[cooks_d > cooks_threshold]

    print(f"\nNumber of points with Cook's Distance > {cooks_threshold:.4f} (threshold 4/N): {len(influential_points_indices)}")
    # Note: Investigating these points requires checking the original data rows by these indices

    # Optional: Plot Pearson residuals vs. fitted values
    plt.figure(figsize=(10, 6))
    # Ensure fittedvalues are used directly - These should also be aligned NumPy arrays
    fitted_values_aligned = final_primary_result.fittedvalues # Removed .loc[common_index]
    plt.scatter(fitted_values_aligned, pearson_residuals, alpha=0.5)
    plt.axhline(0, color='r', linestyle='--')
    plt.xlabel("Fitted Values (Predicted Proportion)")
    plt.ylabel("Pearson Residuals")
    plt.title("Pearson Residuals vs. Fitted Values (Final Model)")
    plt.grid(True, alpha=0.6)
    plt.show()


except Exception as e:
    print(f"Error calculating or plotting residuals/Cook's Distance: {e}")


print("\n--- Final Model Diagnostics Complete ---")
